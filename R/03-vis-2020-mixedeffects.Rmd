---
title: 'Study 3: Mixed Effects Modeling'
subtitle: Vis 2020 Belief Correlation Paper (Karduni et al., 2020)
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## 1. Import Packages

```{r load-packages}
library(ggplot2)
library(statsr)
library(lme4)
library(sjPlot)
library(dplyr)
library(brmstools)
theme_set(theme_sjplot())
```

## 2. Load Data

```{r load-data}
df <- read.csv(file="../data/vis2020/data_exclude.csv")

# refactor and categorize
df$visGroup <- factor(df$visGroup, c("line","band","hop"))
levels(df$visGroup ) <- c("Line","Cone","HOPs")
df$nDataShown <- factor(df$nDataShown)

df <- rename(df, 
             sampleUncertainty = uncertaintyShown,
             visTreatment = visGroup)

df <- within(df, visTreatment <- relevel(visTreatment, ref = 1))
```

## 3. Exploratory Graphs

```{r fig.height=3,fig.width=4}
g1 <- df %>%
  rename(Congruency = congruency) %>%
  ggplot(aes(x=preBeliefDistance,fill=Congruency)) + 
  geom_density(alpha=0.5) +
  annotate("text", x = 1.4, y = 2.8, label = "Incongruent", size = 2.5) +
  annotate("text", x = 0.75, y = 3.7, label = "Congruent", size = 2.5) +
  theme(legend.position = "none") + 
  labs(x = " ", y = " ", subtitle = "Pre-Belief Distance") 

g2 <- df %>%
  ggplot(aes(x=sampleUncertainty,fill=nDataShown)) + 
  geom_density(alpha=0.5) +
  annotate("text", x = 1.3, y = 2, label = "Data Shown\n(n = 10)", size = 2.5) +
  annotate("text", x = 0.75, y = 3.7, label = "Data Shown\n(n = 100)", size = 2.5) +
  theme(legend.position = "none") +
  labs(x = " ", y = " ", subtitle = "Sample Uncertainty")

cowplot::plot_grid(g1, g2,
                   label_x = -0.2,
                   ncol = 2)
```

## 4. Frequentist Mixed Effects Modeling (`lme4`)

```{r lme4-modeling}
# Absolute Belief Distance
m = lmer(diffBeliefAbs ~ visTreatment * preBeliefDistance + visTreatment * sampleUncertainty +  sampleUncertainty * preBeliefDistance  +  (1|usertoken) + (1|vars),df)
# Uncertainty Difference
m1 = lmer(diffUncertainty ~ visTreatment * preBeliefDistance + visTreatment * sampleUncertainty +  sampleUncertainty * preBeliefDistance  + (1|usertoken) + (1|vars),df)
```

```{r}
a <- plot_model(m,show.values = TRUE, vline.color = "grey", value.offset = .4, value.size = 3, type="est", show.intercept = TRUE ) +
  scale_y_continuous(breaks=seq(-.75,0.75,.25)) +
  theme(axis.text.y = element_text(size = 8),
        plot.subtitle=element_text(size=11), plot.title = element_text(size = 1)) +
  labs(subtitle = "Absolute Belief Difference", title = "") +
  ylim(-0.25, 0.9)

b <- plot_model(m1, vline.color = "grey",show.values = TRUE, value.offset = .4, value.size = 3, show.intercept = TRUE) +
  ylim(-.3,.3) +
  theme(axis.text.y=element_blank(),
        plot.subtitle=element_text(size=11), plot.title = element_text(size = 1)) +
  labs(subtitle = "Uncertainty Difference", title = "")

# final plots
library(cowplot)

plot_grid(a,
  b,
  label_x = -0.2,
  ncol = 2,
  rel_widths = c(4.6, 2.4)) 
```

## 5a. Absolute Belief Difference

### Bayesian Mixed Effects

For the Vis 2020 paper, we did not run a Bayesian mixed effects model. 

Let's examine the first regression to estimate the effect on the absolute belief change (`diffBeliefAbs`). We'll use the same functional form as model `m`.

```{r eval=FALSE, include=TRUE}
library(brms)

bm <- brms::brm(diffBeliefAbs ~ visTreatment * preBeliefDistance + visTreatment * sampleUncertainty +  sampleUncertainty * preBeliefDistance + (1|usertoken) + (1|vars), data = df)

save(bm, file = "../models/fit_bm.rda")
```

```{r}
load("../models/fit_bm.rda")
```

First let's look at metadata around the model.

```{r}
coef_bm <- coefplot(bm)
coef_bm
```

```{r}
coef_m_df <- a$data %>% rename(Parameter = term) %>% mutate(Parameter = as.character(Parameter))

coef_bm_df <- coef_bm$data
coef_bm_df$Parameter[coef_bm_df$Parameter=="Intercept"] <- "(Intercept)"

joined_models <- inner_join(coef_bm_df, coef_m_df, by = "Parameter")
```


Notice that the coefficients are very similar to Frequentist:

```{r}
joined_models %>%
  rename(Bayesian_Estimate = Estimate, Freq_Estimate = estimate) %>%
  select(Parameter, Bayesian_Estimate, Freq_Estimate) %>%
  mutate(abs_diff = round(abs(Bayesian_Estimate - Freq_Estimate),3)) %>%
  knitr::kable()
```

We see the same for the coefficients standard errors (though they mean slightly different things):

```{r}
joined_models %>%
  rename(Bayesian_Error = Est.Error, Freq_Error = std.error) %>%
  select(Parameter, Bayesian_Error, Freq_Error) %>%
  mutate(abs_diff_error = round(abs(Bayesian_Error - Freq_Error),3)) %>%
  knitr::kable()
```

### Model convergence / posterior predictive check

The convergence stats also look good - Rhat's are at 1 and we have "fuzzy caterpillars". 

```{r}
plot(bm)
```

But remember - convergence doesn't mean great fit. Let's evaluate overfitting with Posterior Predictive Checks. We'll do 10 draws and compare to actual. 

```{r}
pp_check(bm)
```

There looks like some misspecification.

### Modify response (likelihood) to lognormal

Let's try instead a lognormal likelihood (specifically the `hurdle_lognormal` because we have a handful of cases where diffBeliefAbs equals zero (see [brms comment](https://discourse.mc-stan.org/t/convergence-fails-for-every-truncated-gaussian-model/10040/2)))..

```{r include=TRUE,eval=FALSE}
#df$diffBeliefAbsAdjusted <- ifelse(df$diffBeliefAbs==0,0.01,df$diffBeliefAbs)

bm2 <- brms::brm(diffBeliefAbs ~ visTreatment * preBeliefDistance + visTreatment * sampleUncertainty +  sampleUncertainty * preBeliefDistance + (1|usertoken) + (1|vars), data = df, family = hurdle_lognormal(link = "identity", link_sigma = "log"))

save(bm2, file = "../models/fit_bm2.rda")
```

```{r}
load("../models/fit_bm2.rda")
```

### What are model priors?

```{r}
bm2$prior
```

### What are the coefficients?

```{r}
coef_bm2 <- coefplot(bm2)
coef_bm2
```


### Model Comparison

First, let's use leave-one-out (loo) cross-validation. It will also provide estimate to determine point leverage (aka outliers).

```{r}
looNormal <- loo(bm, save_psis = TRUE)
print(looNormal)
```

```{r}
looNormal <- loo(bm, save_psis = TRUE)
print(looNormal)
```

```{r}
looLog <- loo(bm2, save_psis = TRUE)
print(looLog)
```

When comparing two fitted models, we can estimate the difference in their expected predictive
accuracy by the difference in elpd-dloo or elpd-dwaic.

```{r}
loo_compare(looNormal, looLog)
```

WAIC criterion

```{r}
waicNormal = waic(bm)
waicLog = waic(bm2)
loo_compare(waicNormal, waicLog)
```

As a last step, let's do a posterior predictive check:

```{r}
pp_check(bm2) + xlim(-1,3)
```

Better -- but we're still overfitting. It appears to be bimodal, maybe even "tri"-modal. We suspect this is due to the IV's, namely Belief Distance's bimodal features.


### Compare Coefficients

As a final check, let's compare the coefficients for the normal Bayesian mixed effects model and the (hurdle) Lognormal Bayesian mixed effects model.

```{r}
coef_bm_df <- coef_bm$data
coef_bm2_df <- coef_bm2$data
coef_bm_df$Parameter[coef_bm_df$Parameter=="Intercept"] <- "(Intercept)"
coef_bm2_df$Parameter[coef_bm2_df$Parameter=="Intercept"] <- "(Intercept)"

joined_models <- inner_join(coef_bm_df, coef_bm2_df, by = "Parameter")
```


Let's examine the coefficient differences with the different likelihoods.

```{r}
un_coef <- joined_models %>%
  rename(Normal_Estimate = Estimate.x, Lognormal_Estimate = Estimate.y) %>%
  select(Parameter, Normal_Estimate, Lognormal_Estimate) 

un_error <- joined_models %>%
  rename(Normal_low = `2.5%ile.x`, Normal_high = `97.5%ile.x`,Lognormal_low = `2.5%ile.y`, Lognormal_high = `97.5%ile.y`) %>%
  select(Parameter, Normal_low, Normal_high, Lognormal_low, Lognormal_high) 

var_order <- c("(Intercept)","visTreatmentCone","visTreatmentHOPs","preBeliefDistance","sampleUncertainty","visTreatmentCone:preBeliefDistance","visTreatmentHOPs:preBeliefDistance","visTreatmentCone:sampleUncertainty","visTreatmentHOPs:sampleUncertainty","preBeliefDistance:sampleUncertainty")

inner_join(un_coef,un_error,by="Parameter") %>%
  tidyr::pivot_longer(-Parameter) %>%
  tidyr::separate(name, c("Model","Estimate"), sep = "_") %>%
  tidyr::pivot_wider(names_from = c("Estimate")) %>%
  mutate(Parameter = factor(Parameter, levels = rev(var_order))) %>%
  mutate(Model = factor(Model, levels = c("Normal", "Lognormal"))) %>%
  ggplot(aes(x = Parameter, color = Model)) +
  geom_hline(yintercept = 0, alpha = 0.4) +
  geom_point(aes(y = Estimate),  position=position_dodge(.9)) +
  geom_errorbar(aes(ymin = low, ymax = high),  position=position_dodge(.9)) +
  theme(legend.position = c(0.2,0.2),) +
  labs(title = "Abs Belief Difference", subtitle = "Study 3: Vary by Response Distribution") +
  scale_color_manual(values = c("Lognormal" = "red",
                                "Normal"="black")) +  
  coord_flip()
```

We see the same for the coefficients standard errors:

```{r}
joined_models %>%
  rename(Normal_Error = Est.Error.x, Lognormal_Error = Est.Error.y) %>%
  select(Parameter, Normal_Error, Lognormal_Error) %>%
  mutate(Diff_Error = round(Normal_Error - Lognormal_Error,3)) %>%
  knitr::kable()
```

## 5b. Uncertainty Difference

### Bayesian Mixed Effects

Let's now examine uncertainty difference (`diffUncertainty`). We'll use the same functional form as model `m`.

```{r eval=FALSE,include=TRUE}
bm_u <- brms::brm(diffUncertainty ~ visTreatment * preBeliefDistance + visTreatment * sampleUncertainty +  sampleUncertainty * preBeliefDistance + (1|usertoken) + (1|vars), data = df)

save(bm_u, file = "../models/fit_bm_u.rda")
```

```{r}
load("../models/fit_bm_u.rda")
```

First let's look at metadata around the model.

```{r}
coef_bm_u <- coefplot(bm_u)
coef_bm_u
```

```{r}
coef_mu_df <- b$data %>% rename(Parameter = term) %>% mutate(Parameter = as.character(Parameter))

coef_bm_u <- coefplot(bm_u)

coef_bm_u_df <- coef_bm_u$data
coef_bm_u_df$Parameter[coef_bm_u_df$Parameter=="Intercept"] <- "(Intercept)"

joined_models <- inner_join(coef_bm_u_df, coef_mu_df, by = "Parameter")
```


Notice that the coefficients are very similar to Frequentist:

```{r}
joined_models %>%
  rename(Bayesian_Estimate = Estimate, Freq_Estimate = estimate) %>%
  select(Parameter, Bayesian_Estimate, Freq_Estimate) %>%
  mutate(abs_diff = round(abs(Bayesian_Estimate - Freq_Estimate),3)) %>%
  knitr::kable()
```

We see the same for the coefficients standard errors (though they mean slightly different things):

```{r}
joined_models %>%
  rename(Bayesian_Error = Est.Error, Freq_Error = std.error) %>%
  select(Parameter, Bayesian_Error, Freq_Error) %>%
  mutate(abs_diff_error = round(abs(Bayesian_Error - Freq_Error),3)) %>%
  knitr::kable()
```

### Model convergence / posterior predictive check

The convergence stats also look good - Rhat's are at 1 and we have "fuzzy caterpillars". 

```{r}
plot(bm_u)
```

But remember - convergence doesn't mean great fit. Let's evaluate overfitting with Posterior Predictive Checks. We'll do 10 draws and compare to actual. 

```{r}
pp_check(bm_u)
```

There looks like some misspecification but not terrible (i.e., both symmetric distributions). Let's try a t-distribution instead.

### Modify response (likelihood) to t-distribution

Let's try instead a student t distribution

```{r eval=FALSE,include=TRUE}
bm2_u <- brms::brm(diffUncertainty ~ visTreatment * preBeliefDistance + visTreatment * sampleUncertainty +  sampleUncertainty * preBeliefDistance + (1|usertoken) + (1|vars), data = df, family = student(link = "identity", link_sigma = "log", link_nu = "logm1"))

save(bm2_u, file = "../models/fit_bm2_u.rda")
```

```{r}
load("../models/fit_bm2_u.rda")
```

### What are model priors?

```{r}
bm2_u$prior
```

### What are the coefficients?

```{r}
coef_bm2_u <- coefplot(bm2_u)
coef_bm2_u
```


### Model Comparison

First, let's use leave-one-out (loo) cross-validation. It will also provide estimate to determine point leverage (aka outliers).

```{r}
looNormal_u <- loo(bm_u, save_psis = TRUE)
print(looNormal_u)
```

```{r}
looT <- loo(bm2_u, save_psis = TRUE)
print(looT)
```

When comparing two fitted models, we can estimate the difference in their expected predictive
accuracy by the difference in elpd-dloo or elpd-dwaic.

```{r}
loo_compare(looNormal_u, looT)
```

WAIC criterion

```{r}
waicNormal_u = waic(bm_u)
waicT = waic(bm2_u)
loo_compare(waicNormal_u, waicT)
```

As a last step, let's do a posterior predictive check:

```{r}
pp_check(bm2_u) + xlim(-3,3)
```

### Compare Coefficients

As a final check, let's compare the coefficients for the normal Bayesian mixed effects model and the t-distribution Bayesian mixed effects model.

```{r}
coef_bm_u_df <- coef_bm_u$data
coef_bm2_u_df <- coef_bm2_u$data
coef_bm_u_df$Parameter[coef_bm_u_df$Parameter=="Intercept"] <- "(Intercept)"
coef_bm2_u_df$Parameter[coef_bm2_u_df$Parameter=="Intercept"] <- "(Intercept)"

joined_models <- inner_join(coef_bm_u_df, coef_bm2_u_df, by = "Parameter")
```


```{r}
joined_models %>%
  rename(Normal_Estimate = Estimate.x, StudentT_Estimate = Estimate.y) %>%
  select(Parameter, Normal_Estimate, StudentT_Estimate) %>%
  mutate(Difference = round(Normal_Estimate - StudentT_Estimate,3)) %>%
  knitr::kable()
```

We see the same for the coefficients standard errors:

```{r}
joined_models %>%
  rename(Normal_Error = Est.Error.x, StudentT_Error = Est.Error.y) %>%
  select(Parameter, Normal_Error, StudentT_Error) %>%
  mutate(Diff_Error = round(Normal_Error - StudentT_Error,3)) %>%
  knitr::kable()
```

```{r}
un_coef <- joined_models %>%
  rename(Normal_Estimate = Estimate.x, StudentT_Estimate = Estimate.y) %>%
  select(Parameter, Normal_Estimate, StudentT_Estimate) 

un_error <- joined_models %>%
  rename(Normal_low = `2.5%ile.x`, Normal_high = `97.5%ile.x`,StudentT_low = `2.5%ile.y`, StudentT_high = `97.5%ile.y`) %>%
  select(Parameter, Normal_low, Normal_high, StudentT_low, StudentT_high) 

var_order <- c("(Intercept)","visTreatmentCone","visTreatmentHOPs","preBeliefDistance","sampleUncertainty","visTreatmentCone:preBeliefDistance","visTreatmentHOPs:preBeliefDistance","visTreatmentCone:sampleUncertainty","visTreatmentHOPs:sampleUncertainty","preBeliefDistance:sampleUncertainty")

inner_join(un_coef,un_error,by="Parameter") %>%
  tidyr::pivot_longer(-Parameter) %>%
  tidyr::separate(name, c("Model","Estimate"), sep = "_") %>%
  tidyr::pivot_wider(names_from = c("Estimate")) %>%
  mutate(Parameter = factor(Parameter, levels = rev(var_order))) %>%
    mutate(Model = factor(Model, levels = c("Normal", "StudentT"))) %>%
  ggplot(aes(x = Parameter, color = Model)) +
  geom_hline(yintercept = 0, alpha = 0.4) +
  geom_point(aes(y = Estimate),  position=position_dodge(.9)) +
  geom_errorbar(aes(ymin = low, ymax = high),  position=position_dodge(.9)) +
  theme(legend.position = c(0.2,0.2),) +
  labs(title = "Uncertainty Difference", subtitle = "Study 3: Vary by Response Distribution") +
  scale_color_manual(values = c("StudentT" = "red",
                                "Normal"="black")) + 
  coord_flip()
```


---
title: 'Study 3: Mixed Effects Modeling'
subtitle: Vis 2020 Belief Correlation Paper (Karduni et al., 2020)
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## 1. Import Packages

```{r load-packages}
library(ggplot2)
library(statsr)
library(lme4)
library(sjPlot)
library(dplyr)
library(brmstools)
theme_set(theme_sjplot())
```

## 2. Load Data

```{r load-data}
df <- read.csv(file="../data/vis2020/data_exclude.csv")

# refactor and categorize
df$visGroup <- factor(df$visGroup, c("line","band","hop"))
levels(df$visGroup ) <- c("Line","Cone","HOPs")
df$nDataShown <- factor(df$nDataShown)

df <- rename(df, 
             sampleUncertainty = uncertaintyShown,
             visTreatment = visGroup)

df <- within(df, visTreatment <- relevel(visTreatment, ref = 1))
```

## 3. Load Model

```{r}
load("../models/fit_bm2_u.rda")
```


## 4. Analyze prior

```{r}
set.seed(182)
nsims <- 100
sigma <- 1 / sqrt(rgamma(nsims, 1, rate = 100))
beta0 <- rnorm(nsims, 0, .5)
beta1 <- rlnorm(nsims, 0, .5)
beta2 <- rnorm(nsims, 0, .5)
beta3 <- rnorm(nsims, 0, .5)

r <- ifelse(runif(nsims)>0.3,1L,0L)
beliefDist <- rlnorm(nsims, 0.1,0.1) + rlnorm(nsims, 0.2,0.2) 
uncerSample <- rstudent_t(nsims, df = 3)

dsims <- data.frame(id=1:nsims)

#dsims <- tibble(pre_belief_norm = (df$preBeliefDistance - mean(df$preBeliefDistance))/sd(df$preBeliefDistance))

visTreatment <- sample.int(3, nsims, replace=TRUE)

for(i in 1:nsims){
  this_mu <- beta0[i] + beta1[i]*beliefDist * visTreatment + beta2[i]*uncerSample * visTreatment + beta3[i]*uncerSample * beliefDist
  dsims[paste0(i)] <- this_mu + rlnorm(nrow(dsims), 0, sigma[i])
}

dsl <- dsims %>% 
  tidyr::pivot_longer(`1`:`100`, names_to = "sim", values_to = "sim_weight")

fake  <- merge(dsl, data.frame(id = 1:3, eta = rnorm(3)), by = "id")


dsl %>% 
  ggplot(aes(sim_weight)) + geom_density() + xlim(-10,15)
  # geom_vline(xintercept = log(60), color = "purple", lwd = 1.2, lty = 2) + 
  # theme_bw(base_size = 16) + 
  # annotate("text", x=300, y=0.0022, label= "Monica's\ncurrent weight", 
  #          color = "purple", size = 5) 
```


```{r}
set.seed(54647)
# number of observations
N <- 1E4
# number of group levels
G <- 20 #round(N / 10)
# number of predictors
P <- 5
# regression coefficients
beta <- rnorm(P)

# sampled covariates, group means and fake data
fake <- matrix(rnorm(N * P), ncol = P)
dimnames(fake) <- list(NULL, paste0("x", 1:P))

# fixed effect part and sampled group membership
fake <- transform(
  as.data.frame(fake),
  theta = fake %*% beta,
  g = sample.int(G, N, replace=TRUE)
)

# add random intercept by group
fake  <- merge(fake, data.frame(g = 1:G, eta = rnorm(G)), by = "g")

# linear predictor
fake  <- transform(fake, mu = theta + eta)

# sample Poisson data
fake  <- transform(fake, y = rcauchy(N, exp(mu), 1))

# shuffle order of data rows to ensure even distribution of computational effort
fake <- fake[sample.int(N, N),]

# drop not needed row names
rownames(fake) <- NULL

fake %>%
  ggplot(aes(x = y)) + geom_density() + xlim(-10,20)
```


```{r}
prior_mb2 <- bm2_u$prior

## define a prior on all population-level effects a once
#prior_mb2$prior[1] <- "normal(0,10)"
n = 500
data.frame(x = rlnorm(n, 0.1, 0.8)) %>% ggplot(aes(x = x)) + geom_density()

## define a specific prior on the population-level effect of Trt
#prior_mb2$prior[5] <- "student_t(10, 0, 5)"       

## verify that the priors indeed found their way into Stan's model code
sc <- make_stancode(diffUncertainty ~ visTreatment * preBeliefDistance + visTreatment * sampleUncertainty +  sampleUncertainty * preBeliefDistance + (1|usertoken) + (1|vars), 
              data = df, 
              family = student(link = "identity", link_sigma = "log", link_nu = "logm1"), 
              prior = prior_mb2)

sc
```

```{r}
set.seed(1234)
dat <- data.frame(
  y = c(rnorm(200), rnorm(100, 6)), 
  x = rnorm(300),
  z = sample(0:1, 300, TRUE)
)

ggplot(dat, aes(x, z)) + geom_point()

## fit a simple normal mixture model
mix <- mixture(hurdle_lognormal, hurdle_lognormal)
prior <- c(
  prior(normal(0, 1), Intercept, dpar = mu1),
  prior(normal(0, 1), Intercept, dpar = mu2)
)

get_prior(bf(diffUncertainty ~ visTreatment * preBeliefDistance + visTreatment * sampleUncertainty +  sampleUncertainty * preBeliefDistance + (1|usertoken) + (1|vars)),
          family = mix, data = df)

bprior <- c(prior_string("normal(0,10)", class = "b"),
            prior(student_t(3, 0, 2.5), class = b, coef = sampleUncertainty),
            prior(student_t(3, 0, 2.5), class = b, coef = preBeliefDistance))

fit1 <- brm(bf(diffBeliefAbs ~ visTreatment * preBeliefDistance + visTreatment * sampleUncertainty +  sampleUncertainty * preBeliefDistance + (1|usertoken) + (1|vars)), df, family = mix, chains = 4, cores = 4, backend = "cmdstanr") 

summary(fit1)
pp_check(fit1) + xlim(-2,2)
```